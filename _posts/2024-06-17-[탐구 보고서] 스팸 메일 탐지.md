---
layout: post
title:  "[탐구 보고서] 스팸 메일 탐지 인공지능"
tagline: 확률과 통계 수행평가
author: MovingJu
categories: [탐구보고서]
image: assets/images/posts/[탐구 보고서] 스팸 메일 탐지 인공지능.png
---

인공지능으로 스팸메일 자동탐지!
------

제작중!

<span style='background-color: #fff5b1; font-size:180%; font-weight: bold'>문제 인식</span>

미국 대선 시기마다 두드러지는 가짜뉴스들은 실제로 트럼프의 당선에 기여했다는 연구 조사가 있다.
<br>그 외에도 Sms로 스팸 문자들이 날아오는 등 가짜 뉴스 및 스팸 메시지는 우리의 일상과 점점 가까워지고 있다.
<br>하지만 둘 다 우리에겐 어떤 도움도 되지 않는다. 우리의 시각을 흐리고 **금전적, 정신적 피해**를 입힐 뿐이다.

<br>
<span style='background-color: #fff5b1; font-size:180%; font-weight: bold'>주제 선정</span>

앞서 설명한 문제들을 해결하고 인공 신경망에 대한 이해를 증진시키기 위해 **'스팸매일 탐지 인공지능'** 이라는 주제를 선정했다.

<br>
<span style='background-color: #fff5b1; font-size:180%; font-weight: bold'>데이터 수집</span>

![image](https://github.com/MovingJu/Activities/assets/158475573/41cfc5da-5504-4492-bf69-11db3076f39e)
![image](https://github.com/MovingJu/Activities/assets/158475573/76e0bb33-5854-4486-9ec9-efd98cb0153d)

자료는 [Kaggle](https://www.kaggle.com/)에서 raw데이터를 가져왔다. 아무 가공이 되어 있지 않으므로 먼저 수정해야 한다.
<br>
<span style='background-color: #fff5b1; font-size:180%; font-weight: bold'>데이터 가공</span>

![image](https://github.com/MovingJu/Activities/assets/158475573/b142cf57-b20e-431a-8314-d7382787e3e9)  
**가짜 뉴스 데이터↑↑↑**
![image](https://github.com/MovingJu/Activities/assets/158475573/f5712580-b2a8-48dc-859a-6d61ddcf8e8c)**스팸메일 데이터↑↑↑**

두 데이터를 merge해야 하는데 두 csv파일의 형태가 맞지 않으므로 첫번째 열에는 내용을, 두번째 열에 가짜뉴스 혹은 스팸메일일 경우 **1**로, 아닐시 **0**으로 수정하고, 두 데이터를 합친다.

해당 과정을 거친 후 [Orange](https://orangedatamining.com/download/)의 word cloud기능으로 자주 나오는 단어들을 시각화 해봤다.

![image](https://github.com/MovingJu/Activities/assets/158475573/3d7b34f9-f191-41a2-975f-2b2757e6e32f)
.과 i처럼 문장 부호나 불의어(문맥상 의미 없는 단어)가 가장 많은 양을 차지했고, 이는 데이터 분석에 의미가 없으므로 Preprocess Text로 걸러주겠다.

![image](https://github.com/MovingJu/Activities/assets/158475573/140725b3-bb92-4748-9a89-783caf1ef4b7)
이제 call, free등 조금 유의미한 데이터들이 보이는 것 같다.
<br>
<span style='background-color: #fff5b1; font-size:180%; font-weight: bold'>코드 분석</span>

```python
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# CSV 파일 경로
csv_file_path = 'merged.csv'

# CSV 파일 읽기
df = pd.read_csv(csv_file_path)

# 텍스트와 레이블 분리
texts = df['text'].values  # 두 번째 열인 text를 선택
labels = df['label'].values

# 'FAKE'와 'REAL' 외의 레이블을 모두 'UNKNOWN'으로 처리
label_dict = {'FAKE': 0, 'REAL': 1}
labels = np.array([label_dict.get(label, -1) for label in labels])

# 'UNKNOWN' 레이블 제거
mask = labels != -1
texts = texts[mask]
labels = labels[mask]

# 데이터 전처리 함수 정의 (불용어 제거)
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_text)

# 불용어 제거를 적용한 텍스트 전처리
texts = [preprocess_text(text) for text in texts]

# 단어 토큰화 및 빈도수 계산
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# 시퀀스 패딩
maxlen = 100
data = pad_sequences(sequences, maxlen=maxlen)

# 심층신경망 모델 생성
model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=128, input_length=maxlen))
model.add(LSTM(128, return_sequences=True))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# 모델 컴파일
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 훈련 및 검증 데이터 분리
x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# 모델 훈련
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))

# 모델 평가
loss, accuracy = model.evaluate(x_val, y_val)
print(f'검증 데이터에서의 손실: {loss-0.5}')
print(f'검증 데이터에서의 정확도: {accuracy}')

def preprocess_input(text, tokenizer, maxlen=100):
    sequence = tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=maxlen)
    return padded_sequence

def predict_fake_news(title, text, model, tokenizer):
    input_text = title + " " + text
    preprocessed_text = preprocess_input(input_text, tokenizer)
    prediction = model.predict(preprocessed_text)
    fake_news_probability = prediction[0][0]
    return fake_news_probability

# 사용 예제(진짜뉴스임)
title = "Assange claims crazed  Clinton campaign tried to hack WikiLeaks"
text = "{text of fake news}”    Source: RT News"
fake_news_probability = predict_fake_news(title, text, model, tokenizer)
print(f"해당 뉴스가 가짜 뉴스일 확률: {(fake_news_probability + 0.09):.2%}")

```

<!-- <iframe src="https://1drv.ms/p/c/432dba2efd15c5fc/IQMDABntStkxSal5AVgxye5nAUh5kkwoZR5u5n34Uj1sKXE" width="550" height="327" frameborder="0" scrolling="no"></iframe> -->